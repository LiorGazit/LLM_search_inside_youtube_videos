{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Analyze a Youtube video by asking the LLM\n",
        "By [Lior Gazit](https://www.linkedin.com/in/liorgazit/)  \n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/LiorGazit/LLM_search_inside_youtube_videos/blob/main/Analyze_a_Youtube_video_by_asking_the_LLM.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "**Description of the notebook:**  \n",
        "Pick a Youtube video that you'd like to understand what value it brings you without having to spend the time to watch all of it.  \n",
        "For instance: an hour long lecture about a topic you are looking to learn about, and your goal is know whether it touches on all key points before dedicating time to watch it.  \n",
        "This is with the intuition that if it were a PDF instead of a video, you'd be able to search through it.  \n",
        "\n",
        "**Requirements:**  \n",
        "* Open this notebook in a free [Google Colab instance](https://colab.research.google.com/github/LiorGazit/LLM_search_inside_youtube_videos/blob/main/Analyze_a_Youtube_video_by_asking_the_LLM.ipynb).  \n",
        "* This code picks OpenAI's API as a choice of LLM, so a paid **API key** is necessary.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip -q install youtube-transcript-api\n",
        "%pip -q install openai\n",
        "%pip -q install numpy\n",
        "%pip -q install pytube\n",
        "%pip -q install faiss-cpu\n",
        "%pip -q install tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "import faiss\n",
        "import numpy as np\n",
        "import openai\n",
        "import tiktoken\n",
        "from urllib.parse import urlparse, parse_qs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Insert API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "my_api_key = \"...\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Pick the Youtube Video and Insert its URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "video_url = \"https://www.youtube.com/watch?v=ySEx_Bqxvvo&ab_channel=AlexanderAmini\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Save API Key to Environement Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = my_api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Define functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract video ID from URL\n",
        "def extract_video_id(url):\n",
        "    query = urlparse(url).query\n",
        "    params = parse_qs(query)\n",
        "    return params['v'][0]\n",
        "\n",
        "# Fetch transcript using youtube-transcript-api\n",
        "def get_transcript(video_url):\n",
        "    video_id = extract_video_id(video_url)\n",
        "    transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n",
        "    text = ' '.join([t['text'] for t in transcript])\n",
        "    return text\n",
        "\n",
        "# Split transcript into chunks\n",
        "def split_chunks(transcript, max_tokens=500):\n",
        "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    words = transcript.split()\n",
        "    chunks, current_chunk = [], []\n",
        "\n",
        "    for word in words:\n",
        "        current_chunk.append(word)\n",
        "        if len(encoding.encode(' '.join(current_chunk))) > max_tokens:\n",
        "            current_chunk.pop()\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = [word]\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "    return chunks\n",
        "\n",
        "# Get embeddings using updated OpenAI embeddings model\n",
        "def get_embeddings(chunks, model=\"text-embedding-3-small\"):\n",
        "    embeddings = openai.embeddings.create(\n",
        "        input=chunks,\n",
        "        model=model\n",
        "    )\n",
        "    embeddings_list = [e.embedding for e in embeddings.data]\n",
        "    return np.array(embeddings_list, dtype='float32')\n",
        "\n",
        "# Build FAISS index\n",
        "def build_index(embeddings):\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(embeddings)\n",
        "    return index\n",
        "\n",
        "# Similarity search\n",
        "def search_chunks(question, chunks, index, top_k=3):\n",
        "    query_embedding = openai.embeddings.create(\n",
        "        input=[question],\n",
        "        model=\"text-embedding-3-small\"\n",
        "    ).data[0].embedding\n",
        "    query_embedding = np.array([query_embedding], dtype='float32')\n",
        "\n",
        "    _, indices = index.search(query_embedding, top_k)\n",
        "    return [chunks[i] for i in indices[0]]\n",
        "\n",
        "# Query LLM with retrieved context using the latest GPT-4 model\n",
        "def query_llm(prompt, model=\"gpt-4-turbo\"):\n",
        "    completion = openai.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You answer questions based on video transcripts. Drop a new line after every sentence!\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.5,\n",
        "        max_tokens=1000\n",
        "    )\n",
        "    return completion.choices[0].message.content.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set Up the Retrieval Mechanism:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entire pipeline execution\n",
        "def pipeline(video_url, question):\n",
        "    print(\"--- Prompt ---\\n\")\n",
        "    print(question)\n",
        "\n",
        "    # Fetching transcript:\n",
        "    transcript = get_transcript(video_url)\n",
        "\n",
        "    # Splitting transcript into chunks:\n",
        "    chunks = split_chunks(transcript)\n",
        "\n",
        "    # Getting embeddings:\n",
        "    embeddings = get_embeddings(chunks)\n",
        "\n",
        "    # Building FAISS index:\n",
        "    index = build_index(embeddings)\n",
        "\n",
        "    # Searching relevant chunks:\n",
        "    relevant_chunks = search_chunks(question, chunks, index)\n",
        "\n",
        "    context = \"\\n\\n\".join(relevant_chunks)\n",
        "    prompt = f\"Context from video:\\n\\n{context}\\n\\nQuestion: {question}\\nStart a new line after every sentence in your answer!\"\n",
        "\n",
        "    print(\"\\n--- Answer ---\\n\")\n",
        "    return query_llm(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Some Questions About the Content of the Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Prompt ---\n",
            "\n",
            "Do they mention transformers? In what way? Tell me in 2-3 sentences.\n",
            "\n",
            "--- Answer ---\n",
            "\n",
            "Yes, transformers are mentioned in the context of utilizing self-attention mechanisms within neural networks. The speaker describes transformers as powerful architectures that are built upon the concept of self-attention to process input data efficiently and extract important features. Multiple self-attention heads within transformers help in forming a rich representation of data by focusing on different relevant parts of the input.\n"
          ]
        }
      ],
      "source": [
        "question = \"Do they mention transformers? In what way? Tell me in 2-3 sentences.\"\n",
        "print(pipeline(video_url, question))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Prompt ---\n",
            "\n",
            "Do they mention attention?\n",
            "\n",
            "--- Answer ---\n",
            "\n",
            "Yes, they mention attention multiple times throughout the video. \n",
            "They discuss the concept of attention and self-attention as foundational mechanisms in the Transformer architecture. \n",
            "They explain how attention helps in identifying and focusing on the most important parts of the input data. \n",
            "The video also describes how attention weights are computed and used to extract features that deserve high attention.\n"
          ]
        }
      ],
      "source": [
        "question = \"Do they mention attention?\"\n",
        "print(pipeline(video_url, question))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Prompt ---\n",
            "\n",
            "Do they mention back propogation? Please provide 2-3 sentences that tell about it.\n",
            "\n",
            "--- Answer ---\n",
            "\n",
            "Yes, they mention back propagation in the context of training recurrent neural networks (RNNs). The video explains that back propagation in RNNs involves propagating errors back through each time step in the sequence, which is termed as back propagation through time (BPTT). This process helps in updating the weights of the network to minimize the overall loss, accounting for the temporal dynamics of the input sequences.\n"
          ]
        }
      ],
      "source": [
        "question = \"Do they mention back propogation? Please provide 2-3 sentences that tell about it.\"\n",
        "backprop_answer_english = pipeline(video_url, question)\n",
        "print(backprop_answer_english)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Translate the Last Response to Hindi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "हां, उन्होंने पुनरावर्ती न्यूरल नेटवर्क (RNNs) को प्रशिक्षित करने के संदर्भ में बैक प्रोपेगेशन का उल्लेख किया है।\n",
            "वीडियो समझाता है कि RNNs में बैक प्रोपेगेशन का अर्थ है त्रुटियों को अनुक्रम में प्रत्येक समय चरण के माध्यम से वापस प्रसारित करना, जिसे समय के माध्यम से बैक प्रोपेगेशन (BPTT) कहा जाता है।\n",
            "यह प्रक्रिया नेटवर्क के वजन को अपडेट करने में मदद करती है ताकि कुल हानि को कम किया जा सके, इनपुट अनुक्रमों की समयिक गतिशीलता को ध्यान में रखते हुए।\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"Please translate this answer from English to Hindi: <{backprop_answer_english}>. Make sure to translate properly with the appropriate technical terms.\"\n",
        "print(query_llm(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Translate the Last Response to Tamil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ஆம், அவர்கள் மீளாய்வு நரம்பியல் வலைகள் (RNNs) பயிற்சியின் சூழலில் பின்னோக்கு பரவலை குறிப்பிடுகின்றனர். \n",
            "வீடியோ விளக்கம் அளிக்கிறது என்பது ஆர்.என்.என்-களில் பின்னோக்கு பரவல் என்பது கால அடுக்குகளில் ஒவ்வொரு படியாக பிழைகளை பின்னோக்கிப் பரப்புவதை உள்ளடக்கியது, இது கால வழியாக பின்னோக்கு பரவல் (BPTT) என அழைக்கப்படுகிறது. \n",
            "இந்த செயல்முறை நெட்வொர்க்கின் எடைகளை புதுப்பித்து, மொத்த இழப்பைக் குறைப்பதற்கு உதவுகிறது, உள்ளீட்டு அடுக்குகளின் கால இயக்க விதிமுறைகளை கணக்கில் கொள்கிறது.\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"Please translate this answer from English to Tamil: <{backprop_answer_english}>. Make sure to translate properly with the appropriate technical terms.\"\n",
        "print(query_llm(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Video's Text that the LLM Can Use to Answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello everyone! I hope you enjoyed Alexander's \n",
            "first lecture. I'm Ava and in this second lecture,   Lecture 2, we're going to focus on this \n",
            "question of sequence modeling -- how   we can build neural networks that can \n",
            "handle and learn from sequential data. So in Alexander's first lecture he \n",
            "introduced the essentials of neural   networks starting with perceptrons building \n",
            "up to feed forward models and how you can   actually train these models and start \n",
            "to think about deploying them forward. Now we're going to turn our attention to \n",
            "specific types of problems that involve   sequential processing of data and we'll \n",
            "realize why these types of problems require   a different way of implementing and building \n",
            "neural networks from what we've seen so far. And I think some of the components in \n",
            "this lecture traditionally can be a bit   confusing or daunting at first but what I \n",
            "really really want to do is to build this   understanding up from the foundations walking \n",
            "through step by step developing intuition   all the way to understanding the math and the \n",
            "operations behind how these networks operate. Okay so let's let's get started to to \n",
            "begin I to begin I first want to motivate   what exactly we mean when we talk about \n",
            "sequential data or sequential modeling. So we're going to begin with a really simple \n",
            "intuitive example let's say we have this   picture of a ball and your task is to predict \n",
            "where this ball is going to travel to next. Now if you don't have any prior information about   the trajectory of the ball it's motion \n",
            "it's history any guess or prediction   about its next position is going \n",
            "to be exactly that a random guess. If however in addition to the current \n",
            "location of the ball I gave you some   information about where it was moving in the \n",
            "past now the problem becomes much easier and   I think hopefully we can all agree that most \n",
            "likely or most likely next prediction is that   this ball is going to move forward \n",
            "to the right in in the next frame. So this is a really you know reduced down \n",
            "bare bones intuitive example but the truth   is that beyond this sequential \n",
            "data is really all around us. As I'm speaking the words coming out of \n",
            "my mouth form a sequence of sound waves   that define audio which we can split up \n",
            "to think about in this sequential manner   similarly text language can be split up into a \n",
            "sequence of characters or a sequence of words   and there are many many more examples in which \n",
            "sequential processing sequential data is present   right from medical signals like EKGs to financial \n",
            "markets and projecting stock prices to biological   sequences encoded in DNA to patterns in the \n",
            "climate to patterns of motion and many more   and so already hopefully you're getting \n",
            "a sense of what these types of questions   and problems may look like and where \n",
            "they are relevant in the real world   when we consider applications of sequential \n",
            "modeling in the real world we can think about   a number of different kind of problem definitions \n",
            "that we can have in our Arsenal and work with   in the first lecture Alexander introduced the \n",
            "Notions of classification and the notion of   regression where he talked about and we learned \n",
            "about feed forward models that can operate one   to one in this fixed and static setting right \n",
            "given a single input predict a single output   the binary classification example of \n",
            "will you succeed or pass this class   here there's there's no notion of sequence \n",
            "there's no notion of time now if we introduce   this idea of a sequential component we can \n",
            "handle inputs that may be defined temporally   and potentially also produce a sequential \n",
            "or temporal output so for as one example we   can consider text language and maybe we want to \n",
            "generate one prediction given a sequence of text   classifying whether a message is a \n",
            "positive sentiment or a negative sentiment   conversely we could have a single input let's say \n",
            "an image and our goal may be now to generate text   or a sequential description of this image right \n",
            "given this image of a baseball player throwing a   ball can we build a neural network that generates \n",
            "that as a language caption finally we can also   consider applications and problems where we have \n",
            "sequence in sequence L for example if we want to   translate between two languages and indeed this \n",
            "type of thinking in this type of Architecture   is what powers the task of machine translation \n",
            "in your phones in Google Translate and and many   other examples so hopefully right this has given \n",
            "you a picture of what sequential data looks like   what these types of problem definitions may look \n",
            "like and from this we're going to start and build   up our understanding of what neural networks we \n",
            "can build and train for these types of problems   so first we're going to begin with the notion \n",
            "of recurrence and build up from that to Define   recurrent neural networks and in the last \n",
            "portion of the lecture we'll talk about the   underlying mechanisms underlying the Transformer \n",
            "architectures that are very very very powerful in   terms of handling sequential data but as I said \n",
            "at the beginning right the theme of this lecture   is building up that understanding step by step \n",
            "starting with the fundamentals and the intuition   so to do that we're going to go back revisit \n",
            "the perceptron and move forward from there   right so as Alexander introduced where we \n",
            "studied the perception perceptron in lecture one   the perceptron is defined by this single \n",
            "neural operation where we have some set of   inputs let's say X1 through XM and each of these \n",
            "numbers are multiplied by a corresponding weight   pass through a non-linear activation function \n",
            "that then generates a predicted output y hat   here we can have multiple inputs \n",
            "coming in to generate our output   but still these inputs are not thought of as \n",
            "points in a sequence or time steps in a sequence   even if we scale this perceptron and start \n",
            "to stack multiple perceptrons together to   Define these feed forward neural networks we still \n",
            "don't have this notion of temporal processing or   sequential information even though we are able to \n",
            "translate and convert multiple inputs apply these   weight operations apply this non-linearity \n",
            "to then Define multiple predicted outputs   so taking a look at this diagram right on the left \n",
            "in blue you have inputs on the right in purple you   have these outputs and the green defines the \n",
            "neural the single neural network layer that's   transforming these inputs to the outputs Next \n",
            "Step I'm going to just simplify this diagram I'm   going to collapse down those stack perceptrons \n",
            "together and depict this with this green block   still it's the same operation going \n",
            "on right we have an input Vector being   being transformed to predict this output \n",
            "vector now what I've introduced here which   you may notice is this new variable T right \n",
            "which I'm using to denote a single time step   we are considering an input at a single time \n",
            "step and using our neural network to generate   a single output corresponding to that how could \n",
            "we start to extend and build off this to now   think about multiple time steps and how we could \n",
            "potentially process a sequence of information   well what if we took this diagram all I've done \n",
            "is just rotated it 90 degrees where we still have   this input vector and being fed in producing an \n",
            "output vector and what if we can make a copy of   this network right and just do this operation \n",
            "multiple times to try to handle inputs that are   fed in corresponding to different times right \n",
            "we have an individual time step starting with   t0 and we can do the same thing the same operation \n",
            "for the next time step again treating that as an   isolated instance and keep doing this repeatedly \n",
            "and what you'll notice hopefully is all these   models are simply copies of each other just with \n",
            "different inputs at each of these different time   steps and we can make this concrete right in terms \n",
            "of what this functional transformation is doing   the predicted output at a particular time step \n",
            "y hat of T is a function of the input at that   time step X of T and that function is what is \n",
            "learned and defined by our neural network weights   okay so I've told you that our goal here is \n",
            "Right trying to understand sequential data   do sequential modeling but what could \n",
            "be the issue with what this diagram is   showing and what I've shown you \n",
            "here well yeah go ahead [Music]   exactly that's exactly right so the student's \n",
            "answer was that X1 or it could be related to X   naught and you have this temporal dependence but \n",
            "these isolated replicas don't capture that at all   and that's exactly answers the question perfectly \n",
            "right here a predicted output at a later time   step could depend precisely on inputs at previous \n",
            "time steps if this is truly a sequential problem   with this temporal dependence so how could we \n",
            "start to reason about this how could we Define   a relation that links the Network's computations \n",
            "at a particular time step to Prior history and   memory from previous time steps well what if we \n",
            "did exactly that right what if we simply linked   the computation and the information understood \n",
            "by the network to these other replicas via what   we call a recurrence relation what this means is \n",
            "that something about what the network is Computing   at a particular time is passed on to those \n",
            "later time steps and we Define that according   to this variable H which we call this internal \n",
            "state or you can think of it as a memory term   that's maintained by the neurons and the \n",
            "network and it's this state that's being   passed time set to time step as we read in \n",
            "and and process this sequential information   what this means is that the Network's output \n",
            "its predictions its computations is not only   a function of the input data X but also we have \n",
            "this other variable H which captures this notion   of State captions captures this notion of memory \n",
            "that's being computed by the network and passed on   over time specifically right to walk through this \n",
            "our predicted output y hat of T depends not only   on the input at a time but also this past memory \n",
            "this past state and it is this linkage of temporal   dependence and recurrence that defines this idea \n",
            "of a recurrent neural unit what I've shown is this   this connection that's being unrolled over time \n",
            "but we can also depict this relationship according   to a loop this computation to this internal State \n",
            "variable h of T is being iteratively updated over   time and that's fed back into the neuron the \n",
            "neurons computation in this recurrence relation   this is how we Define these recurrent cells \n",
            "that comprise recurrent neural networks or   and the key here is that we have this this idea of \n",
            "this recurrence relation that captures the cyclic   temporal dependency and indeed it's this idea \n",
            "that is really the intuitive Foundation behind   recurrent neural networks or rnns and so let's \n",
            "continue to build up our understanding from here   and move forward into how we can actually Define \n",
            "the RNN operations mathematically and in code   so all we're going to do is formalize this \n",
            "relationship a little bit more the key idea   here is that the RNN is maintaining the state \n",
            "and it's updating the state at each of these   time steps as the sequence is is processed we \n",
            "Define this by applying this recurrence relation   and what the recurrence relation captures is how \n",
            "we're actually updating that internal State h of t   specifically that state update is exactly like any \n",
            "other neural network operator operation that we've   introduced so far where again we're learning \n",
            "a function defined by a set of Weights w we're   using that function to update the cell State h \n",
            "of t and the additional component the newness   here is that that function depends both on the \n",
            "input and the prior time step h of T minus one   and what you'll know is that this function f sub \n",
            "W is defined by a set of weights and it's the same   set of Weights the same set of parameters \n",
            "that are used time step to time step as the   recurrent neural network processes this temporal \n",
            "information the sequential data okay so the key   idea here hopefully is coming coming through is \n",
            "that this RNN stay update operation takes this   state and updates it each time a sequence is \n",
            "processed we can also translate this to how we   can think about implementing rnns in Python code \n",
            "or rather pseudocode hopefully getting a better   understanding and intuition behind how these \n",
            "networks work so what we do is we just start by   defining an RNN for now this is abstracted away \n",
            "and we start we initialize its hidden State and   we have some sentence right let's say this is \n",
            "our input of Interest where we're interested in   predicting maybe the next word that's occurring in \n",
            "this sentence what we can do is Loop through these   individual words in the sentence that Define our \n",
            "temporal input and at each step as We're looping   through each word in that sentence is fed into \n",
            "the RNN model along with the previous hidden state   and this is what generates a prediction for \n",
            "the next word and updates the RNN state in turn   finally our prediction for the final word in \n",
            "the sentence the word that we're missing is   simply the rnn's output after all the prior \n",
            "words have been fed in through the model   so this is really breaking down how the RNN Works \n",
            "how it's processing the sequential information   and what you've noticed is that the \n",
            "RNN computation includes both this   update to the hidden State as well \n",
            "as generating some predicted output   at the end that is our ultimate \n",
            "goal that we're interested in   and so to walk through this how we're actually \n",
            "generating the output prediction itself   what the RNN computes is given some input vector \n",
            "it then performs this update to the hidden state   and this update to the head and state is just \n",
            "a standard neural network operation just like   we saw in the first lecture where it consists of \n",
            "taking a weight Matrix multiplying that by the   previous hidden State taking another weight Matrix \n",
            "multiplying that by the input at a time step and   applying a non-linearity and in this case right \n",
            "because we have these two input streams the input   data X of T and the previous state H we have these \n",
            "two separate weight matrices that the network is   learning over the course of its training that \n",
            "comes together we apply the non-linearity and   then we can generate an output at a given \n",
            "time step by just modifying the hidden state   using a separate weight Matrix to update this \n",
            "value and then generate a predicted output   and that's what there is to it right \n",
            "that's how the RNN in its single   operation updates both the hidden State \n",
            "and also generates a predicted output   okay so now this gives you the internal working \n",
            "of how the RNN computation occurs at a particular   time step let's next think about how this looks \n",
            "like over time and Define the computational graph   of the RNN as being unrolled or expanded acrost \n",
            "across time so so far the dominant way I've been   showing the rnns is according to this loop-like \n",
            "diagram on the Left Right feeding back in on   itself another way we can visualize and think \n",
            "about rnns is as kind of unrolling this recurrence   over time over the individual time steps in our \n",
            "sequence what this means is that we can take   the network at our first time step and continue \n",
            "to iteratively unroll it across the time steps   going on forward all the way until we process all \n",
            "the time steps in our input now we can formalize   this diagram a little bit more by defining the \n",
            "weight matrices that connect the inputs to the   hidden State update and the weight matrices that \n",
            "are used to update the internal State across time   and finally the weight matrices that Define \n",
            "the the update to generate a predicted output   now recall that in all these cases right for all \n",
            "these three weight matrices add all these time   steps we are simply reusing the same weight \n",
            "matrices right so it's one set of parameters   one set of weight matrices that just process this \n",
            "information sequentially now you may be thinking   okay so how do we actually start to be thinking \n",
            "about how to train the RNN how to define the loss   given that we have this temporal processing in \n",
            "this temporal dependence well a prediction at   an individual time step will simply amount to \n",
            "a computed loss at that particular time step   so now we can compare those predictions time step \n",
            "by time step to the true label and generate a loss   value for those timestamps and finally we can get \n",
            "our total loss by taking all these individual loss   terms together and summing them defining the \n",
            "total loss for a particular input to the RNN   if we can walk through an example of how we \n",
            "implement this RNN in tensorflow starting from   scratch the RNN can be defined as a layer \n",
            "operation and a layer class that Alexander   introduced in the first lecture and so we can \n",
            "Define it according to an initialization of weight   matrices initialization of a hidden state which \n",
            "commonly amounts to initializing these two to zero   next we can Define how we can actually pass \n",
            "forward through the RNN Network to process a given   input X and what you'll notice is in this forward \n",
            "operation the computations are exactly like we   just walked through we first update the hidden \n",
            "state according to that equation we introduced   earlier and then generate a predicted output that \n",
            "is a transformed version of that hidden state   and finally at each time step we return it \n",
            "both the output and the updated hidden State   as this is what is necessary to be stored \n",
            "to continue this RNN operation over time   what is very convenient is that although you \n",
            "could Define your RNN Network and your RNN   layer completely from scratch is that tensorflow \n",
            "abstracts this operation away for you so you can   simply Define a simple RNN according to \n",
            "uh this this call that you're seeing here   um which yeah makes all the the computations \n",
            "very efficient and and very easy   and you'll actually get practice implementing and \n",
            "working with with rnns in today's software lab   okay so that gives us the understanding of \n",
            "rnns and going back to what I what I described   as kind of the problem setups or the problem \n",
            "definitions at the beginning of this lecture   I just want to remind you of the types of sequence \n",
            "modeling problems on which we can apply rnns right   we can think about taking a sequence of \n",
            "inputs producing one predicted output   at the end of the sequence we can think \n",
            "about taking a static single input and   trying to generate text according \n",
            "to according to that single input   and finally we can think about taking a sequence \n",
            "of inputs producing a prediction at every time   step in that sequence and then doing this sequence \n",
            "to sequence type of prediction and translation okay so yeah so so this will \n",
            "be the the foundation for   um the software lab today which will focus \n",
            "on this problem of of many to many processing   and many to many sequential modeling \n",
            "taking a sequence going to a sequence   what is common and what is universal across \n",
            "all these types of problems and tasks that we   may want to consider with rnns is what I like \n",
            "to think about what type of design criteria we   need to build a robust and reliable Network for \n",
            "processing these sequential modeling problems what   I mean by that is what are the characteristics \n",
            "what are the the design requirements that the   RNN needs to fulfill in order to be able \n",
            "to handle sequential data effectively   the first is that sequences can be of different \n",
            "lengths right they may be short they may be   long we want our RNN model or our neural network \n",
            "model in general to be able to handle sequences of   variable lengths secondly and really importantly \n",
            "is as we were discussing earlier that the whole   point of thinking about things through the lens of \n",
            "sequence is to try to track and learn dependencies   in the data that are related over time so \n",
            "our model really needs to be able to handle   those different dependencies which may occur at \n",
            "times that are very very distant from each other next right sequence is all about order right \n",
            "there's some notion of how current inputs depend   on prior inputs and the specific order of the \n",
            "observations we see makes a big effect on what   prediction we may want to generate at the end \n",
            "and finally in order to be able to process this   information effectively our Network needs to be \n",
            "able to do what we call parameter sharing meaning   that given one set of Weights that set of weights \n",
            "should be able to apply to different time steps   in the sequence and still result in a meaningful \n",
            "prediction and so today we're going to focus on   how recurrent neural networks meet these design \n",
            "criteria and how these design criteria motivate   the need for even more powerful architectures \n",
            "that can outperform rnns in sequence modeling   so to understand these criteria very concretely \n",
            "we're going to consider a sequence modeling   problem where given some series of words our task \n",
            "is just to predict the next word in that sentence   so let's say we have this sentence this morning I \n",
            "took my cat for a walk and our task is to predict   the last word in the sentence given the prior \n",
            "words this morning I took my cap for a blank   our goal is to take our RNN Define it and put \n",
            "it to test on this task what is our first step   to doing this well the very very first step before \n",
            "we even think about defining the RNN is how we can   actually represent this information to the network \n",
            "in a way that it can process and understand if we have a model that is processing this data \n",
            "processing this text-based data and wanting to   generate text as the output our problem can arise \n",
            "in that the neural network itself is not equipped   to handle language explicitly right remember \n",
            "that neural networks are simply functional   operators they're just mathematical operations \n",
            "and so we can't expect it right it doesn't have   an understanding from the start of what a word is \n",
            "or what language means which means that we need a   way to represent language numerically so that \n",
            "it can be passed in to the network to process   so what we do is that we need to define \n",
            "a way to translate this text this this   language information into a numerical \n",
            "encoding a vector an array of numbers   that can then be fed in to our neural network and \n",
            "generating a a vector of numbers as its output so now right this raises the question \n",
            "of how do we actually Define this   transformation how can we transform \n",
            "language into this numerical encoding   the key solution and the key way that a \n",
            "lot of these networks work is this notion   and concept of embedding what that means \n",
            "is it it's some transformation that takes   indices or something that can be represented as \n",
            "an index into a numerical Vector of a given size   so if we think about how this idea \n",
            "of embedding works for language data   let's consider a vocabulary of words that we can \n",
            "possibly have in our language and our goal is to   be able to map these individual words in our \n",
            "vocabulary to a numerical Vector of fixed size   one way we could do this is by defining all the \n",
            "possible words that could occur in this vocabulary   and then indexing them assigning a index \n",
            "label to each of these distinct words   a corresponds to index one cat responds to index \n",
            "two so on and so forth and this indexing Maps   these individual words to numbers unique indices \n",
            "what these indices can then Define is what we   call a embedding vector which is a fixed length \n",
            "encoding where we've simply indicated a one value   at the index for that word when we observe \n",
            "that word and this is called a one-hot   embedding where we have this fixed length \n",
            "Vector of the size of our vocabulary and   each instance of the vocabulary corresponds \n",
            "to a one-hot one at the corresponding index   this is a very sparse way to do \n",
            "this and it's simply based on   purely purely count the count index there's \n",
            "no notion of semantic information meaning   that's captured in this vector-based encoding \n",
            "alternatively what is very commonly done is to   actually use a neural network to learn in encoding \n",
            "to learn in embedding and the goal here is that we   can learn a neural network that then captures \n",
            "some inherent meaning or inherent semantics   in our input data and Maps related words or \n",
            "related inputs closer together in this embedding   space meaning that they'll have numerical \n",
            "vectors that are more similar to each other   this concept is really really foundational to \n",
            "how these sequence modeling networks work and   how neural networks work in general okay so with \n",
            "that in hand we can go back to our design criteria   thinking about the capabilities that we desire \n",
            "first we need to be able to handle variable   length sequences if we again want to predict \n",
            "the next word in the sequence we can have short   sequences we can have long sequences we can have \n",
            "even longer sentences and our key task is that we   want to be able to track dependencies across \n",
            "all these different lengths and what we need   what we mean by dependencies is that there could \n",
            "be information very very early on in a sequence   but uh that may not be relevant or come up \n",
            "late until very much later in the sequence   and we need to be able to track these dependencies \n",
            "and maintain this information in our Network   dependencies relate to order and sequences are \n",
            "defined by their order and we know that same words   in a completely different order have completely \n",
            "different meanings right so our model needs to   be able to handle these differences in order and \n",
            "the differences in length that could result in   different predicted outputs okay so hopefully \n",
            "that example going through the example in text   motivates how we can think about transforming \n",
            "input data into a numerical encoding that can   be passed into the RNN and also what are \n",
            "the key criteria that we want to meet in   handling these these types of problems so so \n",
            "far we've painted the picture of rnn's how they   work intuition their mathematical operations and \n",
            "what are the key criteria that they need to meet   the final piece to this is how we actually train \n",
            "and learn the weights in the RNN and that's done   through back propagation algorithm with a bit \n",
            "of a Twist to just handle sequential information if we go back and think about how we train feed \n",
            "forward neural network models the steps break down   in thinking through starting with an input where \n",
            "we first take this input and make a forward pass   through the network going from input to Output the \n",
            "key to back propagation that Alexander introduced   was this idea of taking the prediction and back \n",
            "propagating gradients back through the network   and using this operation to then Define and \n",
            "update the loss with respect to each of the   parameters in the network in order to gradually \n",
            "adjust the parameters the weights of the network   in order to minimize the overall loss now with \n",
            "rnns as we walked through earlier we have this   temporal unrolling which means that we have these \n",
            "individual losses across the individual steps   in our sequence that sum together to comprise \n",
            "the overall loss what this means is that when   we do back propagation we have to now instead of \n",
            "back propagating errors through a single Network   back propagate the loss through \n",
            "each of these individual time steps   and after we back propagate loss through each \n",
            "of the individual time steps we then do that   across all time steps all the way from our current \n",
            "time time T back to the beginning of the sequence   and this is the why this is why this algorithm is \n",
            "called back propagation Through Time right because   as you can see the data and the the predictions \n",
            "and the resulting errors are fed back in time   all the way from where we are currently to \n",
            "the very beginning of the input data sequence so the back propagations through time is actually \n",
            "a very tricky algorithm to implement uh in   practice and the reason for this is if we take a \n",
            "close look looking at how gradients flow across   the RNN what this algorithm involves is that many \n",
            "many repeated computations and multiplications of   these weight matrices repeatedly against each \n",
            "other in order to compute the gradient with   respect to the very first time step we have to \n",
            "make many of these multiplicative repeats of   the weight Matrix why might this be problematic \n",
            "well if this weight Matrix W is very very big   what this can result in is what they call what \n",
            "we call the exploding gradient problem where our   gradients that we're trying to use to optimize our \n",
            "Network do exactly that they blow up they explode   and they get really big and makes it infeasible \n",
            "and not possible to train the network stably what   we do to mitigate this is a pretty simple solution \n",
            "called gradient clipping which effectively scales   back these very big gradients to try to \n",
            "constrain them more in a more restricted way   conversely we can have the instance where the \n",
            "weight matrices are very very small and if these   weight matrices are very very small we end up with \n",
            "a very very small value at the end as a result of   these repeated weight Matrix computations and \n",
            "these repeated um multiplications and this is   a very real problem in rnns in particular where \n",
            "we can lead into this funnel called a Vanishing   gradient where now your gradient has just dropped \n",
            "down close to zero and again you can't train the   network stably now there are particular tools that \n",
            "we can use to implement that we can Implement to   try to mitigate the Spanish ingredient problem \n",
            "and we'll touch on each of these three solutions   briefly first being how we can Define the \n",
            "activation function in our Network and how we   can change the network architecture itself to try \n",
            "to better handle this Vanishing gradient problem   before we do that I want to take just \n",
            "one step back to give you a little more   intuition about why Vanishing gradients can \n",
            "be a real issue for recurrent neural networks   Point I've kept trying to reiterate is this notion \n",
            "of dependency in the sequential data and what   it means to track those dependencies well if the \n",
            "dependencies are very constrained in a small space   not separated out that much by time this repeated \n",
            "gradient computation and the repeated weight   matrix multiplication is not so much of a problem \n",
            "if we have a very short sequence where the words   are very closely related to each other and it's \n",
            "pretty obvious what our next output is going to be   the RNN can use the immediately passed \n",
            "information to make a prediction   and so there are not going to be that many uh \n",
            "that much of a requirement to learn effective   weights if the related information \n",
            "is close to to each other temporally   conversely now if we have a sentence \n",
            "where we have a more long-term dependency   what this means is that we need information from \n",
            "way further back in the sequence to make our   prediction at the end and that gap between what's \n",
            "relevant and where we are at currently becomes   exceedingly large and therefore the vanishing \n",
            "gradient problem is increasingly exacerbated   meaning that we really need to um the RNN becomes \n",
            "unable to connect the dots and establish this   long-term dependency all because of this Vanishing \n",
            "gradient issue so the ways that we can imply the   ways and modifications that we can make to our \n",
            "Network to try to alleviate this problem threefold   the first is that we can simply change \n",
            "the activation functions in each of our   neural network layers to be such that they can \n",
            "effectively try to mitigate and Safeguard from   gradients in instances where from shrinking the \n",
            "gradients in instances where the data is greater   than zero and this is in particular true for the \n",
            "relu activation function and the reason is that in   all instances where X is greater than zero with \n",
            "the relu function the derivative is one and so   that is not less than one and therefore it helps \n",
            "in mitigating The Vanishing gradient problem   another trick is how we initialize the parameters \n",
            "in the network itself to prevent them from   shrinking to zero too rapidly and there are there \n",
            "are mathematical ways that we can do this namely   by initializing our weights to Identity matrices \n",
            "and this effectively helps in practice to prevent   the weight updates to shrink too rapidly to zero \n",
            "however the most robust solution to the vanishing   gradient problem is by introducing a slightly \n",
            "more complicated uh version of the recurrent   neural unit to be able to more effectively track \n",
            "and handle long-term dependencies in the data   and this is this idea of gating and what the \n",
            "idea is is by controlling selectively the flow   of information into the neural unit to be able to \n",
            "filter out what's not important while maintaining   what is important and the key and the most popular \n",
            "type of recurrent unit that achieves this gated   computation is called the lstm or long short term \n",
            "memory Network today we're not going to go into   detail on lstn's their mathematical details their \n",
            "operations and so on but I just want to convey the   key idea and intuitive idea about why these lstms \n",
            "are effective at tracking long-term dependencies   the core is that the lstm is able to um control \n",
            "the flow of information through these gates   to be able to more effectively filter out the \n",
            "unimportant things and store the important things   what you can do is Implement Implement lstms \n",
            "in tensorflow just as you would in RNN but   the core concept that I want you to take away when \n",
            "thinking about the lstm is this idea of controlled   information flow through Gates very briefly the \n",
            "way that lstm operates is by maintaining a cell   State just like a standard RNN and that cell state \n",
            "is independent from what is directly outputted   the way the cell state is updated is according to \n",
            "these Gates that control the flow of information   for getting and eliminating what is irrelevant \n",
            "storing the information that is relevant   updating the cell state in turn and then \n",
            "filtering this this updated cell state to   produce the predicted output just like the \n",
            "standard RNN and again we can train the lstm   using the back propagation Through Time algorithm \n",
            "but the mathematics of how the lstm is defined   allows for a completely uninterrupted flow of the \n",
            "gradients which completely eliminates the well   largely eliminates the The Vanishing \n",
            "gradient problem that I introduced earlier   again we're not if you're if you're interested \n",
            "in learning more about the mathematics and the   details of lstms please come and discuss \n",
            "with us after the lectures but again just   emphasizing the core concept and the \n",
            "intuition behind how the lstm operates   okay so so far where we've out where we've been at \n",
            "we've covered a lot of ground we've gone through   the fundamental workings of rnns the architecture \n",
            "the training the type of problems that they've   been applied to and I'd like to close this \n",
            "part by considering some concrete examples   of how you're going to use rnns in your software \n",
            "lab and that is going to be in the task of Music   generation where you're going to work to build an \n",
            "RNN that can predict the next musical note in a   sequence and use it to generate brand new musical \n",
            "sequences that have never been realized before   so to give you an example of just the quality \n",
            "and and type of output that you can try to aim   towards a few years ago there was a work that \n",
            "trained in RNN on a corpus of classical music   data and famously there's this composer \n",
            "Schubert who uh wrote a famous unfinished   Symphony that consisted of two movements but \n",
            "he was unable to finish his uh his Symphony   before he died so he died and then he left the \n",
            "third movement unfinished so a few years ago a   group trained a RNN based model to actually try to \n",
            "generate the third movement to Schubert's famous   unfinished Symphony given the prior to movements \n",
            "so I'm going to play the result quite right now [Music] okay I I paused it I interrupted it quite \n",
            "abruptly there but if there are any classical   music aficionados out there hopefully you get \n",
            "a appreciation for kind of the quality that was   generated uh in in terms of the music quality and \n",
            "this was already from a few years ago and as we'll   see in the next lectures the and continuing with \n",
            "this theme of generative AI the power of these   algorithms has advanced tremendously since \n",
            "we first played this example um particularly   in you know a whole range of domains which I'm \n",
            "excited to talk about but not for now okay so   you'll tackle this problem head on in today's \n",
            "lab RNN music generation foreign we can think   about the the simple example of input sequence \n",
            "to a single output with sentiment classification   where we can think about for example text like \n",
            "tweets and assigning positive or negative labels   to these these text examples based on the \n",
            "content that that is learned by the network   okay so this kind of concludes the portion on rnns \n",
            "and I think it's quite remarkable that using all   the foundational Concepts and operations \n",
            "that we've talked about so far we've been   able to try to build up networks that handle \n",
            "this complex problem of sequential modeling   but like any technology right and RNN is not \n",
            "without limitations so what are some of those   limitations and what are some potential issues \n",
            "that can arise with using rnns or even lstms   the first is this idea of encoding and and \n",
            "dependency in terms of the the temporal separation   of data that we're trying to process while rnns \n",
            "require is that the sequential information is fed   in and processed time step by time step what that \n",
            "imposes is what we call an encoding bottleneck   right where we have we're trying to encode a lot \n",
            "of content for example a very large body of text   many different words into a single output that \n",
            "may be just at the very last time step how do   we ensure that all that information leading up to \n",
            "that time step was properly maintained and encoded   and learned by the network in practice this is \n",
            "very very challenging and a lot of information   can be lost another limitation is that by \n",
            "doing this time step by time step processing   rnns can be quite slow there is not really \n",
            "an easy way to parallelize that computation   and finally together these components of \n",
            "the encoding bottleneck the requirement to   process this data step by step imposes the biggest \n",
            "problem which is when we talk about long memory   the capacity of the RNN and the lstm is really not \n",
            "that long we can't really handle data of tens of   thousands or hundreds of thousands or even Beyond \n",
            "sequential information that effectively to learn   the complete amount of information and patterns \n",
            "that are present within such a rich data source   and so because of this very recently there's been \n",
            "a lot of attention in how we can move Beyond this   notion of step-by-step recurrent processing \n",
            "to build even more powerful architectures for   processing sequential data to understand how we \n",
            "do how we can start to do this let's take a big   step back right think about the high level goal \n",
            "of sequence modeling that I introduced at the   very beginning given some input a sequence of data \n",
            "we want to build a feature encoding and use our   neural network to learn that and then transform \n",
            "that feature encoding into a predicted output   what we saw is that rnns use this notion \n",
            "of recurrence to maintain order information   processing information time step by time step \n",
            "but as I just mentioned we had these key three   bottlenecks to rnns what we really want to achieve \n",
            "is to go beyond these bottlenecks and Achieve even   higher capabilities in terms of the power of \n",
            "these models rather than having an encoding   bottleneck ideally we want to process information \n",
            "continuously as a continuous stream of information   rather than being slow we want to be able to \n",
            "parallelize computations to speed up processing   and finally of course our main goal is \n",
            "to really try to establish long memory   that can build nuanced and Rich \n",
            "understanding of sequential data   the limitation of rnns that's linked to all \n",
            "these problems and issues in our inability   to achieve these capabilities is that they \n",
            "require this time step by time step processing   so what if we could move beyond that what if we \n",
            "could eliminate this need for recurrence entirely   and not have to process the data time set by time \n",
            "step well a first and naive approach would be   to just squash all the data all the time steps \n",
            "together to create a vector that's effectively   concatenated right the time steps are eliminated \n",
            "there's just one one stream where we have now one   vector input with the data from all time points \n",
            "that's then fed into the model it calculates some   feature vector and then generates some output \n",
            "which hopefully makes sense and because we've   squashed all these time steps together we \n",
            "could simply think about maybe building a   feed forward Network that could that could do this \n",
            "computation well with that we'd eliminate the need   for recurrence but we still have the issues that \n",
            "it's not scalable because the dense feed forward   Network would have to be immensely large defined \n",
            "by many many different connections and critically   we've completely lost our in order information by \n",
            "just squashing everything together blindly there's   no temporal dependence and we're then stuck in \n",
            "our ability to try to establish long-term memory so what if instead we could still think \n",
            "about bringing these time steps together   but be a bit more clever about how we try \n",
            "to extract information from this input data   the key idea is this idea of being able to \n",
            "identify and attend to what is important in   a potentially sequential stream of information and \n",
            "this is the notion of attention or self-attention   which is an extremely extremely powerful Concept \n",
            "in modern deep learning and AI I cannot understate   or I don't know understand overstate I I cannot \n",
            "emphasize enough how powerful this concept is   attention is the foundational mechanism of the \n",
            "Transformer architecture which many of you may   have heard about and it's the the the notion \n",
            "of a transformer can often be very daunting   because sometimes they're presented with these \n",
            "really complex diagrams or deployed in complex   applications and you may think okay how \n",
            "do I even start to make sense of this   at its core though attention the key operation \n",
            "is a very intuitive idea and we're going to in   the last portion of this lecture break \n",
            "it down step by step to see why it's so   powerful and how we can use that as part of \n",
            "a larger neural network like a Transformer   specifically we're going to be talking and \n",
            "focusing on this idea of self-attention   attending to the most important parts of an \n",
            "input example so let's consider an image I   think it's most intuitive to consider an image \n",
            "first this is a picture of Iron Man and if our   goal is to try to extract information from this \n",
            "image of what's important what we could do maybe   is using our eyes naively scan over this image \n",
            "pixel by pixel right just going across the image   however our brains maybe maybe internally they're \n",
            "doing some type of computation like this but you   and I we can simply look at this image and \n",
            "be able to attend to the important parts   we can see that it's Iron Man coming at you \n",
            "right in the image and then we can focus in   a little further and say okay what are the \n",
            "details about Iron Man that may be important   what is key what you're doing is your brain \n",
            "is identifying which parts are attending   to to attend to and then extracting those \n",
            "features that deserve the highest attention   the first part of this problem is really \n",
            "the most interesting and challenging one   and it's very similar to the concept of search \n",
            "effectively that's what search is doing taking   some larger body of information and trying \n",
            "to extract and identify the important parts   so let's go there next how does search work you're \n",
            "thinking you're in this class how can I learn more   about neural networks well in this day and age one \n",
            "thing you may do besides coming here and joining   us is going to the internet having all the videos \n",
            "out there trying to find something that matches   doing a search operation so you have a giant \n",
            "database like YouTube you want to find a video   you enter in your query deep learning and \n",
            "what comes out are some possible outputs   right for every video in the database there is \n",
            "going to be some key information related to the   interview to that to that video let's say the \n",
            "title now to do the search what the task is to   find the overlaps between your query and each \n",
            "of these titles right the keys in the database   what we want to compute is a metric of similarity \n",
            "and relevance between the query and these keys how   similar are they to our desired query and we can \n",
            "do this step by step let's say this first option   of a video about the elegant giant sea turtles \n",
            "not that similar to our query about deep learning   our second option introduction to deep learning \n",
            "the first introductory lecture on this class yes   highly relevant the third option a video about \n",
            "the late and great Kobe Bryant not that relevant   the key operation here is that there is this \n",
            "similarity computation bringing the query   and the key together the final step is now that \n",
            "we've identified what key is relevant extracting   the relevant information what we want to pay \n",
            "attention to and that's the video itself we   call this the value and because the searches \n",
            "is implemented well right we've successfully   identified the relevant video on deep learning \n",
            "that you are going to want to pay attention to   and it's this this idea this intuition of \n",
            "giving a query trying to find similarity   trying to extract the related values \n",
            "that form the basis of self-attention   and how it works in neural networks like \n",
            "Transformers so to go concretely into this   right let's go back now to our text our language \n",
            "example with the sentence our goal is to identify   and attend to features in this input that are \n",
            "relevant to the semantic meaning of the sentence   now first step we have sequence we have order \n",
            "we've eliminated recurrence right we're feeding in   all the time steps all at once we still need a way \n",
            "to encode and capture this information about order   and this positional dependence how this is done is \n",
            "this idea of possession positional encoding which   captures some inherent order information present \n",
            "in the sequence I'm just going to touch on this   very briefly but the idea is related to this \n",
            "idea of embeddings which I introduced earlier   what is done is a neural network layer is \n",
            "used to encode positional information that   captures the relative relationships in \n",
            "terms of order within within this text   that's the high level concept right we're still \n",
            "being able to process these time steps all at once   there is no notion of time step rather the data \n",
            "is singular but still we learned this encoding   that captures the positional order information now \n",
            "our next step is to take this encoding and figure   out what to attend to exactly like that search \n",
            "operation that I introduced with the YouTube   example extracting a query extracting a key \n",
            "extracting a value and relating them to each other   so we use neural network layers to do exactly \n",
            "this given this positional encoding what   attention does is applies a neural network layer \n",
            "transforming that first generating the query   we do this again using a separate neural network \n",
            "layer and this is a different set of Weights a   different set of parameters that then transform \n",
            "that positional embedding in a different way   generating a second output the key and finally \n",
            "this repeat this operation is repeated with a   third layer a third set of Weights generating the \n",
            "value now with these three in hand the key the   the query the key and the value we can compare \n",
            "them to each other to try to figure out where   in that self-input the network should attend \n",
            "to what is important and that's the key idea   behind this similarity metric or what you can \n",
            "think of as an attention score what we're doing   is we're Computing a similarity score between a \n",
            "query and the key and remember that these query   and Qui key values are just arrays of numbers \n",
            "we can Define them as arrays of numbers which   you can think of as vectors in space the query \n",
            "Vector the query values are some Vector the key   the key values are some other vector and \n",
            "mathematically the way that we can compare these   two vectors to understand how similar they are is \n",
            "by taking the dot product and scaling it captures   how similar these vectors are how whether or not \n",
            "they're pointing in the same direction right this   is the similarity metric and if you are familiar \n",
            "with a little bit of linear algebra this is also   known as the cosine similarity operation functions \n",
            "exactly the same way for matrices if we apply this   dot product operation to our query in key matrices \n",
            "key matrices we get this similarity metric out   now this is very very key in defining our next \n",
            "step Computing the attention waiting in terms of   what the network should actually attend to within \n",
            "this input this operation gives us a score which   defines how how the components of the input data \n",
            "are related to each other so given a sentence   right when we compute this similarity score metric \n",
            "we can then begin to think of Weights that Define   the relationship between the sequential the \n",
            "components of the sequential data to each other   so for example in the this example with a text \n",
            "sentence he tossed the tennis ball to serve   the goal with the score is that words in the \n",
            "sequence that are related to each other should   have high attention weights ball related to \n",
            "toss related to tennis and this metric itself   is our attention waiting what we have done is \n",
            "passed that similarity score through a soft Max   function which all it does is it constrains \n",
            "those values to be between 0 and 1. and so   you can think of these as relative scores of \n",
            "relative attention weights finally now that we   have this metric that can captures this notion of \n",
            "similarity and these internal self-relationships   we can finally use this metric to extract \n",
            "features that are deserving of high attention   and that's the exact final step in this \n",
            "self-attention mechanism in that we take that   attention waiting Matrix multiply it by the value \n",
            "and get a transformed transformation of of the   initial data as our output which in turn reflects \n",
            "the features that correspond to high attention all right let's take a breath let's \n",
            "recap what we have just covered so far   the goal with this idea of self-attention \n",
            "the backbone of Transformers is to eliminate   recurrence attend to the most important features \n",
            "in in the input data in an architecture how   this is actually deployed is first we take our \n",
            "input data we compute these positional encodings   the neural network layers are applied three-fold \n",
            "to transform the positional encoding into each   of the key query and value matrices we can \n",
            "then compute the self-attention weight score   according to the up the dot product operation \n",
            "that we went through prior and then self-attend   to these features to these uh information to \n",
            "extract features that deserve High attention what is so powerful about this approach in taking \n",
            "this attention wait putting it together with the   value to extract High attention features is that \n",
            "this operation the scheme that I'm showing on   the right defines a single self-attention head \n",
            "and multiple of these self-attention heads can   be linked together to form larger Network \n",
            "architectures where you can think about   these different heads trying to extract different \n",
            "information different relevant parts of the input   to now put together a very very rich encoding and \n",
            "representation of the data that we're working with   intuitively back to our Ironman example what \n",
            "this idea of multiple self-attention heads   can amount to is that different Salient features \n",
            "and Salient information in the data is extracted   first maybe you consider Iron Man attention had \n",
            "one and you may have additional attention heads   that are picking out other relevant parts of \n",
            "the data which maybe we did not realize before   for example the building or the spaceship \n",
            "in the background that's chasing iron   man and so this is a key building block of many \n",
            "many many many powerful architectures that are out   there today today I again cannot emphasize \n",
            "how enough how powerful this mechanism is   and indeed this this backbone idea of \n",
            "self-attention that you just built up   understanding of is the key operation of some \n",
            "of the most powerful neural networks and deep   learning models out there today ranging from the \n",
            "very powerful language models like gpt3 which are   capable of synthesizing natural language in a \n",
            "very human-like fashion digesting large bodies   of text information to understand relationships \n",
            "in text to models that are being deployed for   extremely impactful applications in biology \n",
            "and Medicine such as Alpha full 2 which uses   this notion of self-attention to look at data \n",
            "of protein sequences and be able to predict the   three-dimensional structure of a protein just \n",
            "given sequence information alone and all the   way even now to computer vision which will be the \n",
            "topic of our next lecture tomorrow where the same   idea of attention that was initially developed in \n",
            "sequential data applications has now transformed   the field of computer vision and again using \n",
            "this key concept of attending to the important   features in an input to build these very rich \n",
            "representations of complex High dimensional data   okay so that concludes lectures for today I \n",
            "know we have covered a lot of territory in a   pretty short amount of time but that is what this \n",
            "boot camp program is all about so hopefully today   you've gotten a sense of the foundations of neural \n",
            "networks in the lecture with Alexander we talked   about rnns how they're well suited for sequential \n",
            "data how we can train them using back propagation   how we can deploy them for different applications \n",
            "and finally how we can move Beyond recurrence to   build this idea of self-attention for \n",
            "building increasingly powerful models   for deep learning in sequence modeling \n",
            "all right hopefully you enjoyed we have   um about 45 minutes left for the for the \n",
            "lab portion and open Office hours in which   we welcome you to ask us questions uh of us \n",
            "and the Tas and to start work on the labs   the information for the labs is is up there \n",
            "thank you so much for your attention foreign\n"
          ]
        }
      ],
      "source": [
        "print(get_transcript(video_url))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
