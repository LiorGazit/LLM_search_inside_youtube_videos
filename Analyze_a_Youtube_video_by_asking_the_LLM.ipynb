{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Analyze a Youtube video by asking the LLM\n",
        "By [Lior Gazit](https://www.linkedin.com/in/liorgazit/)  \n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/LiorGazit/LLM_search_inside_youtube_videos/blob/main/Analyze_a_Youtube_video_by_asking_the_LLM.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "**Description of the notebook:**  \n",
        "Pick a Youtube video that you'd like to understand what value it brings you without having to spend the time to watch all of it.  \n",
        "For instance: an hour long lecture about a topic you are looking to learn about, and your goal is know whether it touches on all key points before dedicating time to watch it.  \n",
        "This is with the intuition that if it were a PDF instead of a video, you'd be able to search through it.  \n",
        "\n",
        "**Requirements:**  \n",
        "* Open this notebook in a free [Google Colab instance](https://colab.research.google.com/).  \n",
        "* This code picks OpenAI's API as a choice of LLM, so a paid **API key** is necessary.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip -q install youtube-transcript-api\n",
        "# !pip -q install openai\n",
        "# !pip -q install numpy\n",
        "# !pip -q install pytube\n",
        "# !pip -q install faiss-cpu\n",
        "# !pip -q install tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "import faiss\n",
        "import numpy as np\n",
        "import openai\n",
        "import tiktoken\n",
        "from urllib.parse import urlparse, parse_qs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Insert API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "my_api_key = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Pick the Youtube Video and Insert its URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "video_url = \"https://www.youtube.com/watch?v=ySEx_Bqxvvo&ab_channel=AlexanderAmini\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Save API Key to Environement Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = my_api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Define functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract video ID from URL\n",
        "def extract_video_id(url):\n",
        "    query = urlparse(url).query\n",
        "    params = parse_qs(query)\n",
        "    return params['v'][0]\n",
        "\n",
        "# Fetch transcript using youtube-transcript-api\n",
        "def get_transcript(video_url):\n",
        "    video_id = extract_video_id(video_url)\n",
        "    transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n",
        "    text = ' '.join([t['text'] for t in transcript])\n",
        "    return text\n",
        "\n",
        "# Split transcript into chunks\n",
        "def split_chunks(transcript, max_tokens=500):\n",
        "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    words = transcript.split()\n",
        "    chunks, current_chunk = [], []\n",
        "\n",
        "    for word in words:\n",
        "        current_chunk.append(word)\n",
        "        if len(encoding.encode(' '.join(current_chunk))) > max_tokens:\n",
        "            current_chunk.pop()\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = [word]\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "    return chunks\n",
        "\n",
        "# Get embeddings using OpenAI embeddings API\n",
        "def get_embeddings(chunks, model=\"text-embedding-ada-002\"):\n",
        "    embeddings = openai.embeddings.create(\n",
        "        input=chunks,\n",
        "        model=model\n",
        "    )\n",
        "    embeddings_list = [e.embedding for e in embeddings.data]\n",
        "    return np.array(embeddings_list, dtype='float32')\n",
        "\n",
        "# Build FAISS index\n",
        "def build_index(embeddings):\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(embeddings)\n",
        "    return index\n",
        "\n",
        "# Similarity search\n",
        "def search_chunks(question, chunks, index, top_k=3):\n",
        "    query_embedding = openai.embeddings.create(\n",
        "        input=[question],\n",
        "        model=\"text-embedding-ada-002\"\n",
        "    ).data[0].embedding\n",
        "    query_embedding = np.array([query_embedding], dtype='float32')\n",
        "\n",
        "    _, indices = index.search(query_embedding, top_k)\n",
        "    return [chunks[i] for i in indices[0]]\n",
        "\n",
        "# Query LLM with retrieved context\n",
        "def query_llm(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    completion = openai.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You answer questions based on video transcripts. Drop a new line after every sentence!\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.5,\n",
        "        max_tokens=1000\n",
        "    )\n",
        "    return completion.choices[0].message.content.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set Up the Retrieval Mechanism:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entire pipeline execution\n",
        "def pipeline(video_url, question):\n",
        "    print(\"--- Prompt ---\\n\")\n",
        "    print(question)\n",
        "\n",
        "    # Fetching transcript:\n",
        "    transcript = get_transcript(video_url)\n",
        "\n",
        "    # Splitting transcript into chunks:\n",
        "    chunks = split_chunks(transcript)\n",
        "\n",
        "    # Getting embeddings:\n",
        "    embeddings = get_embeddings(chunks)\n",
        "\n",
        "    # Building FAISS index:\n",
        "    index = build_index(embeddings)\n",
        "\n",
        "    # Searching relevant chunks:\n",
        "    relevant_chunks = search_chunks(question, chunks, index)\n",
        "\n",
        "    context = \"\\n\\n\".join(relevant_chunks)\n",
        "    prompt = f\"Context from video:\\n\\n{context}\\n\\nQuestion: {question}\\nStart a new line after every sentence in your answer!\"\n",
        "\n",
        "    print(\"Querying LLM...\")\n",
        "    print(\"\\n--- Answer ---\\n\")\n",
        "    return query_llm(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Some Questions About the Content of the Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Prompt ---\n",
            "\n",
            "Do they mention transformers? In what way? Tell me in 2-3 sentences.\n",
            "Querying LLM...\n",
            "\n",
            "--- Answer ---\n",
            "\n",
            "Yes, transformers are mentioned in the video transcript. \n",
            "The speaker explains that attention is the foundational mechanism of the Transformer architecture. \n",
            "The video delves into how attention works in neural networks like Transformers, emphasizing its power and importance in processing information efficiently.\n"
          ]
        }
      ],
      "source": [
        "question = \"Do they mention transformers? In what way? Tell me in 2-3 sentences.\"\n",
        "print(pipeline(video_url, question))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Prompt ---\n",
            "\n",
            "Do they mention attention?\n",
            "Querying LLM...\n",
            "\n",
            "--- Answer ---\n",
            "\n",
            "Yes, attention is mentioned multiple times in the video transcript.  \n",
            "The speaker highlights the importance of attention as a concept in modern deep learning and AI.  \n",
            "Attention is described as the foundational mechanism of the Transformer architecture.  \n",
            "The video explains how attention allows for identifying and attending to important information in a sequential stream of data.  \n",
            "Self-attention, specifically, is focused on as a key concept.  \n",
            "The concept of attending to the most important parts of input examples is discussed.  \n",
            "Multiple powerful neural networks and deep learning models leverage the idea of self-attention.  \n",
            "Attention is used in a variety of applications, from language models to computer vision.  \n",
            "The self-attention mechanism is explained step by step in the lecture.  \n",
            "The video emphasizes the transformative impact of attention in various fields.\n"
          ]
        }
      ],
      "source": [
        "question = \"Do they mention attention?\"\n",
        "print(pipeline(video_url, question))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Prompt ---\n",
            "\n",
            "Do they mention back propogation? Please provide 2-3 sentences that tell about it.\n",
            "Querying LLM...\n",
            "\n",
            "--- Answer ---\n",
            "\n",
            "Yes, the video mentions back propagation. \n",
            "Back propagation through time is a key algorithm discussed in the video. \n",
            "It involves feeding back data, predictions, and errors in time to train the network effectively.\n"
          ]
        }
      ],
      "source": [
        "question = \"Do they mention back propogation? Please provide 2-3 sentences that tell about it.\"\n",
        "backprop_answer_english = pipeline(video_url, question)\n",
        "print(backprop_answer_english)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Translate the Last Response to Hindi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "हाँ, वीडियो में बैक प्रोपेगेशन का उल्लेख है।\n",
            "समय के माध्यम से बैक प्रोपेगेशन एक मुख्य एल्गोरिथ्म है जो वीडियो में चर्चा किया गया है।\n",
            "इसमें डेटा, पूर्वानुमान, और गलतियों को समय के माध्यम से प्रशिक्षित करने के लिए प्रभावी रूप से वापस भेजना शामिल है।\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"Please translate this answer from English to Hindi: <{backprop_answer_english}>. Make sure to translate properly with the appropriate technical terms.\"\n",
        "print(query_llm(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Translate the Last Response to Tamil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ஆம், வீடியோவில் பின்னூட்டம் குறிப்பிடுகின்றது.\n",
            "காலத்தில் பின்னூட்டம் மூல அறிவியல் கருவியில் உரைக்கப்பட்டுள்ளது.\n",
            "நெட்வொர்க்கை செயலாக்க நேரத்தில் தரவு, உரைகள், வழுவானங்களை பின்னூட்டிக் கொள்ளும் முறையில் உரைக்கின்றது.\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"Please translate this answer from English to Tamil: <{backprop_answer_english}>. Make sure to translate properly with the appropriate technical terms.\"\n",
        "print(query_llm(prompt))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
